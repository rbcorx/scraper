notes

scrapy startproject <name>
scrapy crawl <spider name>
scrapy shell '<url>'

SelectorList object, wraps HTML
response.css("title")  # "title::text"
    .extract()
    .extract_first()
    [0].extract()
    .re(<regular expressions>)

view(response)

While perhaps not as popular as CSS selectors,
XPath expressions offer more power because besides navigating the structure,
it can also look at the content. Using XPath, you’re able to select things like:
select the link that contains the text “Next Page”. This makes XPath very fitting to
the task of scraping, and we encourage you to learn XPath even if you already know how to construct CSS selectors,
it will make scraping much easier.

scrapy crawl quotes -o quotes.json

The JSON Lines format is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help doing that at the command-line.

# awesome feature
Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting DUPEFILTER_CLASS.

# check
As yet another example spider that leverages the mechanism of following links, check out the CrawlSpider class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it.

Also, a common pattern is to build an item with data from more than one page, using a trick to pass additional data to the callbacks.

